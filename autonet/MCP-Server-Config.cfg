# MCP Server Configuration
# Author: Fatih E. NAR
# Agentic AI Framework
#
# This configuration file controls which MCP backend to use for context storage.
# By default, it uses the local file-based backend which requires no API keys.
# 
# To use SaaS backends (Anthropic, OpenAI, HuggingFace):
# 1. Uncomment the desired backend section below
# 2. Set the required environment variables (API keys)
# 3. Change backend_type to the desired backend name

[DEFAULT]
# Default backend type: local, anthropic, huggingface, openai
backend_type = local

# Connection timeout in seconds
timeout = 30

# Retry attempts for failed operations
retry_attempts = 3

# Enable debug logging
debug = false

[LOCAL]
# Local MCP server configuration (default)
host = localhost
port = 3000
base_url = http://localhost:3000
enable_persistence = true
storage_path = ./mcp_storage
max_contexts = 10000

# Uncomment and configure the backends below when you want to use SaaS endpoints
# Remember to set the corresponding environment variables for API keys

# [ANTHROPIC]
# # Anthropic MCP SaaS endpoint configuration
# base_url = https://api.anthropic.com/v1/mcp
# api_key = ${ANTHROPIC_API_KEY}
# model = claude-3-sonnet-20240229
# max_tokens = 4096
# context_window = 200000
# # Supported context operations
# supports_create = true
# supports_query = true
# supports_update = true
# supports_delete = true
# # Rate limiting
# rate_limit_requests_per_minute = 100
# rate_limit_tokens_per_minute = 50000

# [HUGGINGFACE]
# # HuggingFace MCP SaaS endpoint configuration
# base_url = https://api-inference.huggingface.co/models
# api_key = ${HUGGINGFACE_API_KEY}
# model = microsoft/DialoGPT-large
# # Default model for context processing
# context_model = sentence-transformers/all-MiniLM-L6-v2
# # Inference options
# task = text-generation
# use_cache = true
# wait_for_model = true
# # Supported context operations
# supports_create = true
# supports_query = true
# supports_update = false
# supports_delete = false
# # Rate limiting
# rate_limit_requests_per_minute = 30
# rate_limit_daily_quota = 1000

# [OPENAI]
# # OpenAI MCP SaaS endpoint configuration
# base_url = https://api.openai.com/v1
# api_key = ${OPENAI_API_KEY}
# model = gpt-4-turbo-preview
# organization = ${OPENAI_ORG_ID}
# max_tokens = 4096
# temperature = 0.1
# # Context processing model
# embedding_model = text-embedding-3-large
# # Supported context operations
# supports_create = true
# supports_query = true
# supports_update = true
# supports_delete = true
# # Rate limiting
# rate_limit_requests_per_minute = 500
# rate_limit_tokens_per_minute = 150000

[BACKUP]
# Backup backend configuration
enable_backup = true
backup_backend = local
backup_interval_seconds = 300
backup_on_failure = true

[SECURITY]
# Security settings
enable_encryption = true
encryption_key = ${MCP_ENCRYPTION_KEY}
enable_auth = false
auth_token = ${MCP_AUTH_TOKEN}
allowed_clients = *

[LOGGING]
# Logging configuration
log_level = INFO
log_file = mcp_backend.log
enable_metrics = true
metrics_interval = 60

[CACHE]
# Caching configuration
enable_cache = true
cache_ttl_seconds = 3600
cache_max_size = 1000
cache_backend = memory

[FEDERATION]
# Multi-backend federation support
enable_federation = false
primary_backend = local
fallback_backends = 
load_balancing = round_robin
health_check_interval = 30